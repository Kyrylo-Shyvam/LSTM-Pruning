{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d30d570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:57:47.815549Z",
     "iopub.status.busy": "2023-11-15T17:57:47.814792Z",
     "iopub.status.idle": "2023-11-15T17:58:24.184672Z",
     "shell.execute_reply": "2023-11-15T17:58:24.183544Z"
    },
    "papermill": {
     "duration": 36.377237,
     "end_time": "2023-11-15T17:58:24.187018",
     "exception": false,
     "start_time": "2023-11-15T17:57:47.809781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LSTM-Pruning'...\r\n",
      "remote: Enumerating objects: 569, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (126/126), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (60/60), done.\u001b[K\r\n",
      "remote: Total 569 (delta 74), reused 114 (delta 63), pack-reused 443\u001b[K\r\n",
      "Receiving objects: 100% (569/569), 572.66 MiB | 18.76 MiB/s, done.\r\n",
      "Resolving deltas: 100% (280/280), done.\r\n",
      "Updating files: 100% (75/75), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone -b experiments https://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@github.com/Kyrylo-Shyvam/LSTM-Pruning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560b26ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:58:24.205380Z",
     "iopub.status.busy": "2023-11-15T17:58:24.204603Z",
     "iopub.status.idle": "2023-11-15T17:58:24.211118Z",
     "shell.execute_reply": "2023-11-15T17:58:24.210267Z"
    },
    "papermill": {
     "duration": 0.017616,
     "end_time": "2023-11-15T17:58:24.213041",
     "exception": false,
     "start_time": "2023-11-15T17:58:24.195425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/LSTM-Pruning/code\n"
     ]
    }
   ],
   "source": [
    "%cd ./LSTM-Pruning/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7fda23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:58:24.230392Z",
     "iopub.status.busy": "2023-11-15T17:58:24.230106Z",
     "iopub.status.idle": "2023-11-15T17:58:25.155794Z",
     "shell.execute_reply": "2023-11-15T17:58:25.154571Z"
    },
    "papermill": {
     "duration": 0.936926,
     "end_time": "2023-11-15T17:58:25.158112",
     "exception": false,
     "start_time": "2023-11-15T17:58:24.221186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv ./work_dir/model3.bin ./work_dir/model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f99f6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:58:25.175741Z",
     "iopub.status.busy": "2023-11-15T17:58:25.175420Z",
     "iopub.status.idle": "2023-11-15T17:58:30.698672Z",
     "shell.execute_reply": "2023-11-15T17:58:30.697894Z"
    },
    "papermill": {
     "duration": 5.534638,
     "end_time": "2023-11-15T17:58:30.700979",
     "exception": false,
     "start_time": "2023-11-15T17:58:25.166341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import lstmModel\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1ba07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:58:30.718695Z",
     "iopub.status.busy": "2023-11-15T17:58:30.718342Z",
     "iopub.status.idle": "2023-11-15T17:58:31.110421Z",
     "shell.execute_reply": "2023-11-15T17:58:31.109583Z"
    },
    "papermill": {
     "duration": 0.403371,
     "end_time": "2023-11-15T17:58:31.112794",
     "exception": false,
     "start_time": "2023-11-15T17:58:30.709423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=lstmModel.NMT.load('./work_dir/model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aebabbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:58:31.130811Z",
     "iopub.status.busy": "2023-11-15T17:58:31.130263Z",
     "iopub.status.idle": "2023-11-15T17:58:32.703304Z",
     "shell.execute_reply": "2023-11-15T17:58:32.702482Z"
    },
    "papermill": {
     "duration": 1.584056,
     "end_time": "2023-11-15T17:58:32.705289",
     "exception": false,
     "start_time": "2023-11-15T17:58:31.121233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"14dded5f079435f64fb5e2f0278662dda5605f9e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927864e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:58:32.723849Z",
     "iopub.status.busy": "2023-11-15T17:58:32.723522Z",
     "iopub.status.idle": "2023-11-15T17:58:32.727459Z",
     "shell.execute_reply": "2023-11-15T17:58:32.726601Z"
    },
    "papermill": {
     "duration": 0.01516,
     "end_time": "2023-11-15T17:58:32.729283",
     "exception": false,
     "start_time": "2023-11-15T17:58:32.714123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentages=[0.8,0.9]\n",
    "types=['random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8048af2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:58:32.747165Z",
     "iopub.status.busy": "2023-11-15T17:58:32.746897Z",
     "iopub.status.idle": "2023-11-15T18:20:42.873232Z",
     "shell.execute_reply": "2023-11-15T18:20:42.872353Z"
    },
    "papermill": {
     "duration": 1330.137757,
     "end_time": "2023-11-15T18:20:42.875521",
     "exception": false,
     "start_time": "2023-11-15T17:58:32.737764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "save model parameters to [./work_dir/model.bin.pruned]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvanshikadhingra1030\u001b[0m (\u001b[33moptimalbraindamage\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/LSTM-Pruning/code/wandb/run-20231115_175837-ijwq0xfs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msummer-pyramid-11\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/optimalbraindamage/ablation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/optimalbraindamage/ablation/runs/ijwq0xfs\u001b[0m\n",
      "load test source sentences from [./data/test.de-en.de]\n",
      "load test target sentences from [./data/test.de-en.en]\n",
      "load model from ./work_dir/model.bin.pruned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 6750/6750 [10:20<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "Corpus BLEU: 0.13152608682869657\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bleu ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bleu 0.13153\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msummer-pyramid-11\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/optimalbraindamage/ablation/runs/ijwq0xfs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231115_175837-ijwq0xfs/logs\u001b[0m\n",
      "save model parameters to [./work_dir/model.bin.pruned]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/LSTM-Pruning/code/wandb/run-20231115_180944-c67pyom8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwarm-firebrand-12\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/optimalbraindamage/ablation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/optimalbraindamage/ablation/runs/c67pyom8\u001b[0m\n",
      "load test source sentences from [./data/test.de-en.de]\n",
      "load test target sentences from [./data/test.de-en.en]\n",
      "load model from ./work_dir/model.bin.pruned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 6750/6750 [10:22<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "Corpus BLEU: 0.11498589200668059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bleu ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bleu 0.11499\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwarm-firebrand-12\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/optimalbraindamage/ablation/runs/c67pyom8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231115_180944-c67pyom8/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for p in percentages:\n",
    "    for t in types:\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        lstmModel.pruneFunction({'MODEL_PATH':'./work_dir/model.bin',\n",
    "                                 'PRUNING_TYPE': t,\n",
    "                                 'PERCENTAGE': p})\n",
    "        wandb.init(project=\"ablation\")\n",
    "        wandb.config.pruningType = t\n",
    "        wandb.config.percentage = p\n",
    "        wandb.config.model='model3'\n",
    "        bleu = lstmModel.decode({\n",
    "            '--beam-size': '5',\n",
    "            '--cuda': 'True',\n",
    "            '--max-decoding-time-step': '100',\n",
    "            'MODEL_PATH': './work_dir/model.bin.pruned',\n",
    "            'TEST_SOURCE_FILE': './data/test.de-en.de',\n",
    "            'TEST_TARGET_FILE': './data/test.de-en.en',\n",
    "            'OUTPUT_FILE': './work_dir/decode.txt'\n",
    "        })\n",
    "        wandb.log({\"bleu\": bleu})\n",
    "        \n",
    "        # Finish the run for this combination of p and t\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb6351c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T18:20:44.018425Z",
     "iopub.status.busy": "2023-11-15T18:20:44.017578Z",
     "iopub.status.idle": "2023-11-15T18:31:15.549271Z",
     "shell.execute_reply": "2023-11-15T18:31:15.547994Z"
    },
    "papermill": {
     "duration": 632.080504,
     "end_time": "2023-11-15T18:31:15.551822",
     "exception": false,
     "start_time": "2023-11-15T18:20:43.471318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save results to work_dir\r\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "load test source sentences from [data/test.de-en.de]\r\n",
      "load test target sentences from [data/test.de-en.en]\r\n",
      "load model from work_dir/model.bin.pruned\r\n",
      "Decoding: 100%|█████████████████████████████| 6750/6750 [10:17<00:00, 10.93it/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \r\n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\r\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\r\n",
      "  warnings.warn(_msg)\r\n",
      "Corpus BLEU: 0.11498589200668059\r\n",
      "lastr\r\n",
      "Use of uninitialized value in division (/) at multi-bleu.perl line 139, <STDIN> line 6750.\r\n",
      "Use of uninitialized value in division (/) at multi-bleu.perl line 139, <STDIN> line 6750.\r\n",
      "Use of uninitialized value in division (/) at multi-bleu.perl line 139, <STDIN> line 6750.\r\n",
      "BLEU = 0.00, 0.0/0.0/0.0/0.0 (BP=1.000, ratio=5.147, hyp_len=675000, ref_len=131141)\r\n"
     ]
    }
   ],
   "source": [
    "! ./scripts/decode.sh model.bin.pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d7b6b",
   "metadata": {
    "papermill": {
     "duration": 0.86418,
     "end_time": "2023-11-15T18:31:17.230405",
     "exception": false,
     "start_time": "2023-11-15T18:31:16.366225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30580,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2015.915495,
   "end_time": "2023-11-15T18:31:20.373270",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-15T17:57:44.457775",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
