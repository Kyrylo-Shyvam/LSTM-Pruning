{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport pickle\nimport sys\nimport time\nfrom collections import namedtuple\n\nimport numpy as np\nfrom typing import List, Tuple, Dict, Set, Union\nfrom docopt import docopt\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n\nfrom utils import read_corpus, batch_iter, LabelSmoothingLoss\nfrom vocab import Vocab, VocabEntry\n\n\nHypothesis = namedtuple('Hypothesis', ['value', 'score'])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NMT(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2, input_feed=True, label_smoothing=0.):\n        super(NMT, self).__init__()\n        self.embed_size = embed_size \n        self.hidden_size = hidden_size\n        self.dropout_rate = dropout_rate\n        self.vocab = vocab \n        self.input_feed = input_feed \n        #input feed = true is used when we want to inconporate attentional effects we want to have information not only about the last hidden layer but all hidden layers\n        '''Mentioned in paper: The model will be aware of previous alignment choices.'''\n        # initialize neural network layers\n        #len(vocab.src) gives the no of distinct words in the source vocab\n        self.src_embed = nn.Embedding(len(vocab.src), embed_size, padding_idx=vocab.src['<pad>'])#layer for generating source embedding\n        self.tgt_embed = nn.Embedding(len(vocab.tgt), embed_size, padding_idx=vocab.tgt['<pad>'])#layer for generating target embedding\n        \n        #we are using a bidirectional lstm as The position of specific information in sentences varies across languages due to differences in word order, grammar, and linguistic structures. \n        #this leads to more computation as we have to compute in both forward and backward direction\n        self.encoder_lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True)#hidden_size is the no of neurons in the hidden layer\n        \n        #it takes both the current word's embedding and the previous hidden state as input to the LSTM. \n        decoder_lstm_input = embed_size + hidden_size if self.input_feed else embed_size\n        self.decoder_cell_init =  nn.Linear(hidden_size * 2, hidden_size)\n        \n        #lstm cells constitute lstm\n        self.decoder_lstm = nn.LSTMCell(decoder_lstm_input, hidden_size)#we need to use hidden state after every step for calculating the context vector\n        self.pred = nn.Linear(hidden_size, len(vocab.tgt), bias=False)# prediction layer of the target vocabulary\n        self.dropout = nn.Dropout(self.dropout_rate)\n        \n        # bi-directional to normal size\n        self.att_src_linear = nn.Linear(hidden_size * 2, hidden_size, bias=False)\n        # Projection of context(2h, context of src encodings) + hidden, to 1h attention (also incorporated to input feeding)\n        self.att_vec_linear = nn.Linear(hidden_size * 2 + hidden_size, hidden_size, bias=False)\n        \n        self.label_smoothing = label_smoothing\n        if label_smoothing > 0.:\n            self.label_smoothing_loss = LabelSmoothingLoss(label_smoothing,\n                                                           tgt_vocab_size=len(vocab.tgt), padding_idx=vocab.tgt['<pad>'])\n            \n    @property\n    def device(self) -> torch.device:\n        return self.src_embed.weight.device   \n    \n    #function for encoding the input returns the hidden states and decoder_init_state\n    def encode(self, src_sents_var: torch.Tensor, src_sent_lens: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"\n        src_sents_var is are the list of source sentence tokens\n        src_sent_lens is list giving the lengths of sentences in the batch\n        for eg\n        Sentence 1: \"I like pizza.\"\n        Sentence 2: \"She enjoys pasta.\"\n        Assuming a simplified vocabulary where each word is represented by a unique integer:\n        \"I\" is 1,\"like\" is 2,\"pizza\" is 3,\"She\" is 4,\"enjoys\" is 5,\"pasta\" is 6\n        src_sents_var = torch.Tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (3, 2)\n        src_sent_lens = [3, 3]\n        so src_sents_var is of shape (src_sent_len,batch_size)\n        src_word_embeds = [\n            [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],\n            [[0.7, 0.8, 0.9], [0.1, 0.2, 0.3]],\n            [[0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]\n        ]\n        shape is (3,2,3)\n        basically (src_sent_len, batch_size, embed_size)\n        \"\"\"\n        #when we convert src_sents_var to src_words_embeds the shape is (src_sent_len, batch_size, embed_size)\n        src_word_embeds = self.src_embed(src_sents_var)\n        #suppose the size of differnt sentences are different then we pad the sequences otherwise batch computation would cause problems\n        packed_src_embed = pack_padded_sequence(src_word_embeds, src_sent_lens)\n\n        # src_encodings: (src_sent_len, batch_size, hidden_size * 2) this hidden_size*2 is because of using bidirectional lstm\n        #last_state is the last hidden state shape->(batch_size, hidden_size*2)\n        #last_cell is the last cell state shape->(batch_size,hidden_size*2)\n        src_encodings, (last_state, last_cell) = self.encoder_lstm(packed_src_embed)\n        \n        #unpack the source encodings\n        src_encodings, _ = pad_packed_sequence(src_encodings)\n\n        # (batch_size, src_sent_len, hidden_size * 2) \n        src_encodings = src_encodings.permute(1, 0, 2)\n        #last_cell[0]: The final cell state of the forward LSTM, which has processed the source sentence from left to right.\n        #last_cell[1]: The final cell state of the backward LSTM, which has processed the source sentence from right to left.\n        # shape->(batch_size, 2 * hidden_size)\n        dec_init_cell = self.decoder_cell_init(torch.cat([last_cell[0], last_cell[1]], dim=1))\n        dec_init_state = torch.tanh(dec_init_cell)\n        #src encodings are the hidden state for tokens in the input sequence\n        return src_encodings, (dec_init_state, dec_init_cell)\n    \n    #function to remove the effect of attention from the padded values\n    def get_attention_mask(self, src_encodings: torch.Tensor, src_sents_len: List[int]) -> torch.Tensor:\n        src_sent_masks = torch.zeros(src_encodings.size(0), src_encodings.size(1), dtype=torch.float)\n        for e_id, src_len in enumerate(src_sents_len):\n            src_sent_masks[e_id, src_len:] = 1 #what ever has been padded is set to 1 in this mask\n\n        return src_sent_masks.to(self.device)\n    \n    def decode(self, src_encodings: torch.Tensor, src_sent_masks: torch.Tensor,\n               decoder_init_vec: Tuple[torch.Tensor, torch.Tensor], tgt_sents_var: torch.Tensor) -> torch.Tensor:\n        #transforms the size of src_encoding from (src_sent_len, batch_size, hidden_size * 2) to (src_sent_len, batch_size, hidden_size)\n        src_encoding_att_linear = self.att_src_linear(src_encodings)\n        batch_size = src_encodings.size(0)\n\n        # initialize the attentional vector\n        att_tm1 = torch.zeros(batch_size, self.hidden_size, device=self.device)\n\n        # (tgt_sent_len, batch_size, embed_size)\n        # here we omit the last word, which is always </s>.\n        # Note that the embedding of </s> is not used in decoding\n        tgt_word_embeds = self.tgt_embed(tgt_sents_var)\n\n        h_tm1 = decoder_init_vec\n\n        att_ves = []\n\n        # start from y_0=`<s>`, iterate until y_{T-1}\n        for y_tm1_embed in tgt_word_embeds.split(split_size=1):\n            y_tm1_embed = y_tm1_embed.squeeze(0)\n            if self.input_feed:\n                # input feeding: concate y_tm1 and previous attentional vector\n                # (batch_size, hidden_size + embed_size)\n\n                x = torch.cat([y_tm1_embed, att_tm1], dim=-1)\n            else:\n                x = y_tm1_embed\n\n            (h_t, cell_t), att_t, alpha_t = self.step(x, h_tm1, src_encodings, src_encoding_att_linear, src_sent_masks)\n\n            att_tm1 = att_t\n            h_tm1 = h_t, cell_t\n            att_ves.append(att_t)\n\n        # (tgt_sent_len - 1, batch_size, tgt_vocab_size)\n        att_ves = torch.stack(att_ves)\n\n        return att_ves\n\n    #function for what happens at each step of decoding \n    def step(self, x: torch.Tensor,\n             h_tm1: Tuple[torch.Tensor, torch.Tensor],\n             src_encodings: torch.Tensor, src_encoding_att_linear: torch.Tensor, src_sent_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n        \n        # h_t: (batch_size, hidden_size)\n        #hidden state and cell state of lstm\n        h_t, cell_t = self.decoder_lstm(x, h_tm1)\n        \n        #getting context vector and a_t that is the probability given to each encoder hidden state \n        ctx_t, alpha_t = self.dot_prod_attention(h_t, src_encodings, src_encoding_att_linear, src_sent_masks)\n\n        #getting attentional hidden state\n        att_t = torch.tanh(self.att_vec_linear(torch.cat([h_t, ctx_t], 1)))  # E.q. (5)\n        att_t = self.dropout(att_t)\n\n        return (h_t, cell_t), att_t, alpha_t\n    \n    #function for getting the score using dot product attention\n    def dot_prod_attention(self, h_t: torch.Tensor, src_encoding: torch.Tensor, src_encoding_att_linear: torch.Tensor,\n                           mask: torch.Tensor=None) -> Tuple[torch.Tensor, torch.Tensor]:\n        # (batch_size, src_sent_len)\n        #getting the scores by multiplying hidden layer of decoder and hidden layer of encoder all collectively\n        scores = torch.bmm(src_encoding_att_linear, h_t.unsqueeze(2)).squeeze(2)\n        \n        #setting scores to -infinity incase the bool mask is 1 as we set 1 mask for all the padded values\n        if mask is not None:\n            scores.data.masked_fill_(mask.bool(), -float('inf'))\n        #getting probabilities for each hidden state\n        a_t = F.softmax(scores, dim=-1)\n\n        att_view = (scores.size(0), 1, scores.size(1))\n        \n        # (batch_size, hidden_size)\n        #getting the context vector by weighted multiplication of encoder hidden states and a_t\n        ctx_vec = torch.bmm(a_t.view(*att_view), src_encoding).squeeze(1)\n\n        return ctx_vec, a_t\n    \n    def forward(self, src_sents: List[List[str]], tgt_sents: List[List[str]]) -> torch.Tensor:\n        # (src_sent_len, batch_size)\n        src_sents_var = self.vocab.src.to_input_tensor(src_sents, device=self.device)\n        # (tgt_sent_len, batch_size)\n        tgt_sents_var = self.vocab.tgt.to_input_tensor(tgt_sents, device=self.device)\n        src_sents_len = [len(s) for s in src_sents]\n\n        src_encodings, decoder_init_vec = self.encode(src_sents_var, src_sents_len)\n\n        src_sent_masks = self.get_attention_mask(src_encodings, src_sents_len)\n\n        # (tgt_sent_len - 1, batch_size, hidden_size)\n        att_vecs = self.decode(src_encodings, src_sent_masks, decoder_init_vec, tgt_sents_var[:-1])\n\n        # (tgt_sent_len - 1, batch_size, tgt_vocab_size)\n        tgt_words_log_prob = F.log_softmax(self.pred(att_vecs), dim=-1)\n\n        if self.label_smoothing:\n            # (tgt_sent_len - 1, batch_size)\n            tgt_gold_words_log_prob = self.label_smoothing_loss(tgt_words_log_prob.view(-1, tgt_words_log_prob.size(-1)),\n                                                                tgt_sents_var[1:].view(-1)).view(-1, len(tgt_sents))\n        else:\n            # (tgt_sent_len, batch_size)\n            tgt_words_mask = (tgt_sents_var != self.vocab.tgt['<pad>']).float()\n\n            # (tgt_sent_len - 1, batch_size)\n            tgt_gold_words_log_prob = torch.gather(tgt_words_log_prob, index=tgt_sents_var[1:].unsqueeze(-1), dim=-1).squeeze(-1) * tgt_words_mask[1:]\n\n        # (batch_size)\n        scores = tgt_gold_words_log_prob.sum(dim=0)\n\n        return scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n        \"\"\"\n        Given a single source sentence, perform beam search\n\n        Args:\n            src_sent: a single tokenized source sentence\n            beam_size: beam size\n            max_decoding_time_step: maximum number of time steps to unroll the decoding RNN\n\n        Returns:\n            hypotheses: a list of hypothesis, each hypothesis has two fields:\n                value: List[str]: the decoded target sentence, represented as a list of words\n                score: float: the log-likelihood of the target sentence\n        \"\"\"\n\n        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n\n        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n        src_encodings_att_linear = self.att_src_linear(src_encodings)\n\n        h_tm1 = dec_init_vec\n        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n\n        eos_id = self.vocab.tgt['</s>']\n\n        hypotheses = [['<s>']]\n        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n        completed_hypotheses = []\n\n        t = 0\n        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n            t += 1\n            hyp_num = len(hypotheses)\n\n            exp_src_encodings = src_encodings.expand(hyp_num,\n                                                     src_encodings.size(1),\n                                                     src_encodings.size(2))\n\n            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n                                                                           src_encodings_att_linear.size(1),\n                                                                           src_encodings_att_linear.size(2))\n\n            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n            y_tm1_embed = self.tgt_embed(y_tm1)\n\n            if self.input_feed:\n                x = torch.cat([y_tm1_embed, att_tm1], dim=-1)\n            else:\n                x = y_tm1_embed\n\n            (h_t, cell_t), att_t, alpha_t = self.step(x, h_tm1,\n                                                      exp_src_encodings, exp_src_encodings_att_linear, src_sent_masks=None)\n\n            # log probabilities over target words\n            log_p_t = F.log_softmax(self.readout(att_t), dim=-1)\n\n            live_hyp_num = beam_size - len(completed_hypotheses)\n            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n\n            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n\n            new_hypotheses = []\n            live_hyp_ids = []\n            new_hyp_scores = []\n\n            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n                prev_hyp_id = prev_hyp_id.item()\n                hyp_word_id = hyp_word_id.item()\n                cand_new_hyp_score = cand_new_hyp_score.item()\n\n                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n                if hyp_word == '</s>':\n                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n                                                           score=cand_new_hyp_score))\n                else:\n                    new_hypotheses.append(new_hyp_sent)\n                    live_hyp_ids.append(prev_hyp_id)\n                    new_hyp_scores.append(cand_new_hyp_score)\n\n            if len(completed_hypotheses) == beam_size:\n                break\n\n            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n            att_tm1 = att_t[live_hyp_ids]\n\n            hypotheses = new_hypotheses\n            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n\n        if len(completed_hypotheses) == 0:\n            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n                                                   score=hyp_scores[0].item()))\n\n        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n\n        return completed_hypotheses\n\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@staticmethod\ndef load(model_path: str):\n    params = torch.load(model_path, map_location=lambda storage, loc: storage)\n    args = params['args']\n    model = NMT(vocab=params['vocab'], **args)\n    model.load_state_dict(params['state_dict'])\n\n    return model\n\ndef save(self, path: str):\n    print('save model parameters to [%s]' % path, file=sys.stderr)\n\n    params = {\n        'args': dict(embed_size=self.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate,\n                     input_feed=self.input_feed, label_smoothing=self.label_smoothing),\n        'vocab': self.vocab,\n        'state_dict': self.state_dict()\n    }\n\n    torch.save(params, path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef evaluate_ppl(model, dev_data, batch_size=32):\n    \"\"\"\n        perplexity is exponential of average negative log likelihood\n    \"\"\"\n\n    was_training = model.training\n    model.eval()\n\n    cum_loss = 0.\n    cum_tgt_words = 0.\n\n    with torch.no_grad():\n        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n            # forward pass returns a vector of size batch size containing log likelihood of each sentence in the batch we sum over all the losses and multiply by -1 to get negative of log likelihood that is loss\n            loss = -model(src_sents, tgt_sents).sum()\n            \n            cum_loss += loss.item()\n            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n            cum_tgt_words += tgt_word_num_to_predict\n        \n        ppl = np.exp(cum_loss / cum_tgt_words)\n\n    if was_training:\n        model.train()\n\n    return ppl\n\ndef compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n    if references[0][0] == '<s>':\n        references = [ref[1:-1] for ref in references]\n\n    bleu_score = corpus_bleu([[ref] for ref in references],\n                             [hyp.value for hyp in hypotheses])\n\n    return bleu_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(args: Dict):\n    #appending <s> and </s> to all sentences\n    train_data_src = read_corpus(args['--train-src'], source='src')\n    train_data_tgt = read_corpus(args['--train-tgt'], source='tgt')\n\n    dev_data_src = read_corpus(args['--dev-src'], source='src')\n    dev_data_tgt = read_corpus(args['--dev-tgt'], source='tgt')\n    #preparing train data and dev data\n    train_data = list(zip(train_data_src, train_data_tgt))\n    dev_data = list(zip(dev_data_src, dev_data_tgt))\n    \n    train_batch_size = int(args['--batch-size'])\n    clip_grad = float(args['--clip-grad'])\n    valid_niter = int(args['--valid-niter'])\n    log_every = int(args['--log-every'])\n    model_save_path = args['--save-to']\n\n    vocab = Vocab.load(args['--vocab'])\n\n    #defining the model\n    model = NMT(embed_size=int(args['--embed-size']),\n                hidden_size=int(args['--hidden-size']),\n                dropout_rate=float(args['--dropout']),\n                input_feed=args['--input-feed'],\n                label_smoothing=float(args['--label-smoothing']),\n                vocab=vocab)\n    #switch to training mode\n    model.train()\n\n    #doing uniform initialisation we need to try other initialisatin too\n    uniform_init = float(args['--uniform-init'])\n    if np.abs(uniform_init) > 0.:\n        print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n        for p in model.parameters():\n            p.data.uniform_(-uniform_init, uniform_init)\n\n    vocab_mask = torch.ones(len(vocab.tgt))\n    vocab_mask[vocab.tgt['<pad>']] = 0\n\n    device = torch.device(\"cuda:0\" if args['--cuda'] else \"cpu\")\n    print('use device: %s' % device, file=sys.stderr)\n\n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=float(args['--lr']))\n\n    num_trial = 0\n    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n    cum_examples = report_examples = epoch = valid_num = 0\n    hist_valid_scores = []\n    train_time = begin_time = time.time()\n    print('begin Maximum Likelihood training')\n\n    while True:\n        epoch += 1\n\n        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n            train_iter += 1\n\n            optimizer.zero_grad()\n\n            batch_size = len(src_sents)\n\n            # (batch_size)\n            example_losses = -model(src_sents, tgt_sents)\n            batch_loss = example_losses.sum()\n            loss = batch_loss / batch_size\n\n            loss.backward()\n\n            # clip gradient to prevent exploding gradients\n            grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), clip_grad)\n\n            optimizer.step()\n\n            batch_losses_val = batch_loss.item()\n            report_loss += batch_losses_val\n            cum_loss += batch_losses_val\n\n            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n            report_tgt_words += tgt_words_num_to_predict\n            cum_tgt_words += tgt_words_num_to_predict\n            report_examples += batch_size\n            cum_examples += batch_size\n\n            if train_iter % log_every == 0:\n                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n                      'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n                                                                                         report_loss / report_examples,\n                                                                                         math.exp(report_loss / report_tgt_words),\n                                                                                         cum_examples,\n                                                                                         report_tgt_words / (time.time() - train_time),\n                                                                                         time.time() - begin_time), file=sys.stderr)\n\n                train_time = time.time()\n                report_loss = report_tgt_words = report_examples = 0.\n\n            # perform validation\n            if train_iter % valid_niter == 0:\n                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n                                                                                         cum_loss / cum_examples,\n                                                                                         np.exp(cum_loss / cum_tgt_words),\n                                                                                         cum_examples), file=sys.stderr)\n\n                cum_loss = cum_examples = cum_tgt_words = 0.\n                valid_num += 1\n\n                print('begin validation ...', file=sys.stderr)\n\n                # compute dev. ppl and bleu\n                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n                valid_metric = -dev_ppl\n\n                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n                #we try out many models and take the best one intially ther is no model so first conditon is for that\n                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n                hist_valid_scores.append(valid_metric)\n        \n                if is_better:\n                    patience = 0\n                    print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n                    model.save(model_save_path)\n\n                    # also save the optimizers' state\n                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n                elif patience < int(args['--patience']):\n                    patience += 1\n                    print('hit patience %d' % patience, file=sys.stderr)\n\n                    if patience == int(args['--patience']):\n                        num_trial += 1\n                        print('hit #%d trial' % num_trial, file=sys.stderr)\n                        if num_trial == int(args['--max-num-trial']):\n                            print('early stop!', file=sys.stderr)#so if we get worse more than patience no of times we early stop it\n                            exit(0)\n\n                        # decay lr, and restore from previously best checkpoint\n                        lr = optimizer.param_groups[0]['lr'] * float(args['--lr-decay'])\n                        print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n\n                        # load model\n                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n                        model.load_state_dict(params['state_dict'])\n                        model = model.to(device)\n\n                        print('restore parameters of the optimizers', file=sys.stderr)\n                        optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n\n                        # set new lr\n                        for param_group in optimizer.param_groups:\n                            param_group['lr'] = lr\n\n                        # reset patience\n                        patience = 0\n\n                if epoch == int(args['--max-epoch']):\n                    print('reached maximum number of epochs!', file=sys.stderr)\n                    exit(0)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n    was_training = model.training\n    model.eval()\n\n    hypotheses = []\n    with torch.no_grad():\n        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n\n            hypotheses.append(example_hyps)\n\n    if was_training: model.train(was_training)\n\n    return hypotheses","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode(args: Dict[str, str]):\n    \n    print(f\"load test source sentences from [{args['TEST_SOURCE_FILE']}]\", file=sys.stderr)\n    test_data_src = read_corpus(args['TEST_SOURCE_FILE'], source='src')\n    if args['TEST_TARGET_FILE']:\n        print(f\"load test target sentences from [{args['TEST_TARGET_FILE']}]\", file=sys.stderr)\n        test_data_tgt = read_corpus(args['TEST_TARGET_FILE'], source='tgt')\n\n    print(f\"load model from {args['MODEL_PATH']}\", file=sys.stderr)\n    model = NMT.load(args['MODEL_PATH'])\n\n    if args['--cuda']:\n        model = model.to(torch.device(\"cuda:0\"))\n    # get the beam sized hypothesis using beam search\n    hypotheses = beam_search(model, test_data_src,\n                             beam_size=int(args['--beam-size']),\n                             max_decoding_time_step=int(args['--max-decoding-time-step']))\n\n    if args['TEST_TARGET_FILE']:\n        top_hypotheses = [hyps[0] for hyps in hypotheses]\n        #get the bleu_score on test data\n        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n        print(f'Corpus BLEU: {bleu_score}', file=sys.stderr)\n\n    with open(args['OUTPUT_FILE'], 'w') as f:\n        for src_sent, hyps in zip(test_data_src, hypotheses):\n            top_hyp = hyps[0]\n            hyp_sent = ' '.join(top_hyp.value)\n            f.write(hyp_sent + '\\n')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    args = docopt(__doc__)\n\n    # seed the random number generators\n    seed = int(args['--seed'])\n    torch.manual_seed(seed)\n    if args['--cuda']:\n        torch.cuda.manual_seed(seed)\n    np.random.seed(seed * 13 // 7)\n\n    if args['train']:\n        train(args)\n    elif args['decode']:\n        decode(args)\n    else:\n        raise RuntimeError(f'invalid run mode')\n\n\nif __name__ == '__main__':\n    main()","metadata":{},"execution_count":null,"outputs":[]}]}